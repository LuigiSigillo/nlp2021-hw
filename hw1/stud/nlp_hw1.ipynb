{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia di nlp_hw1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "qU-26r3m8IRE",
        "DyM39Hb89WdF",
        "0y_g6ETA_lf3",
        "CRni2aJ-O_VY",
        "91Syok3T4Cmt"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuigiSigillo/nlp2021-hw/blob/master/hw1/stud/nlp_hw1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qU-26r3m8IRE"
      },
      "source": [
        "# Initial setup for both methodologies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bW-CADDzV91j"
      },
      "source": [
        "##  Pre-Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqmOAYIa_elL"
      },
      "source": [
        "#@title Setup of parameters{ run: \"auto\" }\n",
        "WE_LENGTH = \"50\" #@param [50,100,200,300]\n",
        "METHOD_FIRST_APPROACH = \"avg\" #@param [\"avg\",\"sum\"]\n",
        "USE_SEP = False #@param [\"True\", \"False\"] {type:\"raw\"}\n",
        "WORDS_LIMIT = 400000 #@param {type:\"slider\", min:20000, max:400000, step:20000}\n",
        "REMOVE_STOPWORDS = True #@param [\"True\", \"False\"] {type:\"raw\"}\n",
        "LEMMATIZATION = True #@param [\"True\", \"False\"] {type:\"raw\"}\n",
        "LOWERED = False #@param [\"True\", \"False\"] {type:\"raw\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyM39Hb89WdF"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2n5xn1F5kvq"
      },
      "source": [
        "from google.colab import drive\n",
        "# general\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from collections import Counter, defaultdict\n",
        "from tqdm.notebook import tqdm\n",
        "from typing import *\n",
        "import string\n",
        "import json\n",
        "\n",
        "# torch\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import SGD\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# NLTK\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "# SKLEARN\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7lIQtGqAoLS"
      },
      "source": [
        "Code to download and move the glove embeddings in the right folder\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPQc6F08AlBP"
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "root_folder = '/content/drive/My Drive/NLP/nlp2021-hw1'\n",
        "dataset_folder = os.path.join(root_folder,'data')\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "#! wget http://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
        "#! unzip -d data/glove.6B\n",
        "#! cd '/content/drive/My Drive/NLP/nlp2021-hw1'\n",
        "#!unzip '/content/drive/My Drive/NLP/nlp2021-hw1/glove.6B.zip'\n",
        "# !mv glove.6B.200d.txt '/content/drive/My Drive/NLP/nlp2021-hw1/model'\n",
        "# !ls \"{root_folder}/model/\"\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anwpXc08XRKf"
      },
      "source": [
        "##GloVe word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgljP56n-Wgm"
      },
      "source": [
        "Added to the dictionary also the \"UNK\" and \"SEP\" words using a random vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Te3_zheKQQDe"
      },
      "source": [
        "class GloVEEmbedding():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.word_vectors = dict()\n",
        "\n",
        "\n",
        "    def get_word_vectors(self):\n",
        "        with open(root_folder+'/model/glove.6B.'+WE_LENGTH+'d.txt') as f:\n",
        "            for i, line in tqdm(enumerate(f), total=WORDS_LIMIT):\n",
        "                if i == WORDS_LIMIT:\n",
        "                    break\n",
        "                word, *vector = line.strip().split(' ')\n",
        "                vector = torch.tensor([float(c) for c in vector])\n",
        "                \n",
        "                self.word_vectors[word] = vector\n",
        "\n",
        "        self.word_vectors[\"UNK\"] = torch.rand(int(WE_LENGTH))\n",
        "\n",
        "        if USE_SEP:\n",
        "            self.word_vectors[\"SEP\"] = torch.rand(int(WE_LENGTH))\n",
        "        return self.word_vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkMP7JR-DGfB"
      },
      "source": [
        "## Dataset class and interface"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ti80ryeyXUXT"
      },
      "source": [
        "class SentencesDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, dataset_path: str, sentence2vector: Callable, remove_stop_words: bool, lemmatization: bool):\n",
        "        self.data_store = []\n",
        "        self.lemmatization = lemmatization\n",
        "        self.remove_stop_words = remove_stop_words\n",
        "        self.init_structure(dataset_path, sentence2vector)\n",
        "\n",
        "\n",
        "    def init_structure(self, dataset_path: str, sentence2vector) -> None:\n",
        "\n",
        "        with open(dataset_path) as f:\n",
        "            for json_string in f:\n",
        "                single_json = json.loads(json_string)\n",
        "\n",
        "                keyword = single_json['sentence1'][int(single_json['start1']):int(single_json['end1'])]\n",
        "                keyword2 = single_json['sentence2'][int(single_json['start2']):int(single_json['end2'])]\n",
        "                lemma = single_json['lemma']\n",
        "\n",
        "                sep = \" \" if not USE_SEP else \" SEP \"\n",
        "                \n",
        "                if self.lemmatization:\n",
        "                    lemmatized1 = self.use_only_lemma(single_json['sentence1'],lemma,keyword)\n",
        "                    lemmatized2 = self.use_only_lemma(single_json['sentence2'],lemma,keyword2)\n",
        "                else:\n",
        "                    lemmatized1 = single_json['sentence1']\n",
        "                    lemmatized2 = single_json['sentence2']\n",
        "                \n",
        "                if LOWERED:\n",
        "                    lemmatized1 = lemmatized1.lower()\n",
        "                    lemmatized2 = lemmatized2.lower()\n",
        "                    keyword = keyword.lower()\n",
        "                    keyword2 = keyword2.lower()\n",
        "                    lemma = lemma.lower()\n",
        "                \n",
        "                if self.remove_stop_words:\n",
        "                    lemmatized1_without_stop = self.remove_stopwords(lemmatized1,lemma)\n",
        "                    lemmatized2_without_stop = self.remove_stopwords(lemmatized2,lemma)\n",
        "                    sentence =  lemmatized1_without_stop + sep + lemmatized2_without_stop\n",
        "                    # substitue digits with \"number\"\n",
        "                    sentence = self.handle_digits(sentence)\n",
        "                else:\n",
        "                    sentence = lemmatized1 + sep + lemmatized2\n",
        "\n",
        "                if USE_SEP:\n",
        "                    indices = self.get_kwd_indices(sentence,[keyword,keyword2,lemma])\n",
        "                else:\n",
        "                    indices = self.get_kwd_indices(lemmatized1_without_stop,[keyword,keyword2,lemma]) + [42] +self.get_kwd_indices(lemmatized2_without_stop,[keyword,keyword2,lemma])\n",
        "\n",
        "                ground_t = np.float32(1) if single_json['label'] =='True' else np.float32(0)\n",
        "\n",
        "                vector = sentence2vector(sentence,METHOD_FIRST_APPROACH,keyword)\n",
        "                \n",
        "                if vector is None or len(indices)!=3:\n",
        "                    print(sentence,indices, keyword) \n",
        "                    continue\n",
        "                    \n",
        "                self.data_store.append((vector,ground_t,indices))\n",
        "\n",
        "\n",
        "\n",
        "    '''\n",
        "    Substitute every digits with the word \"number\"\n",
        "    '''\n",
        "    def handle_digits(self,sent: str) -> str:\n",
        "        filtered_sentence = [w if w.isalpha() else \"number\" for w in sent.split(\" \") ]\n",
        "        return \" \".join(filtered_sentence)       \n",
        "\n",
        "    '''\n",
        "    Removing the stopwords and the punctuation but there is the possibility that the keyword is contained\n",
        "    inside the set of stopwords so I remove it first.\n",
        "    '''\n",
        "    def remove_stopwords(self,sent: str,lemma: str) -> str:\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        try:\n",
        "            stop_words.remove(lemma)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # remove punkt\n",
        "        others = \"–\" +\"—\" + \"−\" + \"’\" + \"”\" + \"“\" #These chars arent inside the standard punctuation\n",
        "        str_punkt = string.punctuation+ others\n",
        "        translator = str.maketrans(str_punkt, ' '*len(str_punkt)) \n",
        "        word_tokens = word_tokenize(sent.translate(translator)) \n",
        "        \n",
        "        filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "        return \" \".join(filtered_sentence)\n",
        "\n",
        "    '''\n",
        "    Lemmatization of the sentence\n",
        "    '''\n",
        "    def use_only_lemma(self,sent:str ,lemma: str, keyword: str) -> str:\n",
        "        filtered_sentence = [w if not w == keyword else lemma for w in sent.split(\" \") ]\n",
        "        return \" \".join(filtered_sentence)\n",
        "\n",
        "    '''\n",
        "    The indices of the keyword are retrieved, we have to handle the fact that the keywords can be repeated inside the sentence.\n",
        "    So if we are using the separator we use this word SEP to separate the sentences and retrieve the correct indices.\n",
        "    The list returned must contain three elements if we are using the separator: \n",
        "     - index of first occurence of keyword in first sentence\n",
        "     - index of sep\n",
        "     - index of first occurence of keyword in second sentence\n",
        "    '''\n",
        "    def get_kwd_indices(self,sentence: str,keywords: Sequence[str]) -> Sequence[int]:\n",
        "        i = 0\n",
        "        j_list = []\n",
        "        sec = False\n",
        "        sentence_list = sentence.split(\" \")\n",
        "        if USE_SEP:\n",
        "            while i < len(sentence_list):\n",
        "                if sentence_list[i] == \"SEP\":\n",
        "                    sec = True\n",
        "                    j_list.append(i)\n",
        "                if sentence_list[i] in keywords:\n",
        "                    if j_list == []:\n",
        "                        j_list.append(i)\n",
        "                    elif sec:\n",
        "                        j_list.append(i)\n",
        "                        return j_list\n",
        "                i += 1\n",
        "        else: # single sentence\n",
        "            while i < len(sentence_list):\n",
        "                if sentence_list[i] in keywords:\n",
        "                    j_list.append(i)\n",
        "                    return j_list\n",
        "                i+= 1\n",
        "        return j_list\n",
        "            \n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data_store)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
        "        return self.data_store[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxSfhuYWSPOh"
      },
      "source": [
        "Versatile datamodule taht can be used for both approaches, since what is changing is the sentence2vector function and the collate one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8K2TtSOXY13"
      },
      "source": [
        "class SentencesDataModule(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self, \n",
        "        data_train_path: str,\n",
        "        data_dev_path: str,\n",
        "        batch_size: int,\n",
        "        vectorize_function: Callable,\n",
        "        collate_fn=None\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.data_train_path = data_train_path\n",
        "        self.data_dev_path = data_dev_path\n",
        "        self.batch_size = batch_size\n",
        "        self.collate_fn = collate_fn\n",
        "        self.vectorize_function = vectorize_function\n",
        "\n",
        "        self.train_dataset = None\n",
        "        self.validation_dataset = None\n",
        "\n",
        "    def train_dataloader(self, *args, **kwargs) -> DataLoader:\n",
        "        self.train_dataset = SentencesDataset(self.data_train_path, self.vectorize_function, REMOVE_STOPWORDS, LEMMATIZATION)\n",
        "        return DataLoader(self.train_dataset, batch_size=self.batch_size, collate_fn=self.collate_fn)\n",
        "\n",
        "    def val_dataloader(self, *args, **kwargs) -> Union[DataLoader, List[DataLoader]]:\n",
        "        self.validation_dataset = SentencesDataset(self.data_dev_path, self.vectorize_function, REMOVE_STOPWORDS, LEMMATIZATION)\n",
        "        return DataLoader(self.validation_dataset, batch_size=self.batch_size,collate_fn=self.collate_fn)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0y_g6ETA_lf3"
      },
      "source": [
        "# First approach **(word-level)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKed-2p_XsMH"
      },
      "source": [
        "Word-embedding-powered function. Converts sentences to a vector by averaging the embeddings corresponding to each word in it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpsIPHyUV6Lt"
      },
      "source": [
        "def sentence2vector(sentence: str, method: str, keyword: str) -> Optional[torch.Tensor]:\n",
        "    sentences_word_vector = []\n",
        "    if method == \"avg\":\n",
        "        sentences_word_vector = [word_vectors[w] if w in word_vectors else word_vectors['UNK'] for w in sentence.split(' ')]\n",
        "    elif method ==\"weigthed_avg\":\n",
        "        for w in sentence.split(' '):\n",
        "            coeff = 1\n",
        "            if w in word_vectors:\n",
        "                if w == keyword:\n",
        "                    coeff = 1.5\n",
        "                sentences_word_vector.append(word_vectors[w]*coeff)\n",
        "            else:\n",
        "                sentences_word_vector.append(word_vectors['UNK'])\n",
        "    if len(sentences_word_vector) == 0:\n",
        "        return None\n",
        "\n",
        "    sentences_word_vector = torch.stack(sentences_word_vector)  # tensor shape: (#words X #features)\n",
        "    if method==\"sum\":\n",
        "        return torch.sum(sentences_word_vector, dim=0)\n",
        "    else:\n",
        "        return torch.mean(sentences_word_vector, dim=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELkIYmAwDTft"
      },
      "source": [
        "Loading and testing of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwuHz1kdXryl"
      },
      "source": [
        "BATCH_SIZE = 40 #@param {type:\"slider\", min:8, max:64, step:8}\n",
        "\n",
        "glove_embed = GloVEEmbedding()\n",
        "word_vectors = glove_embed.get_word_vectors()\n",
        "\n",
        "sentences_dm = SentencesDataModule(\n",
        "    data_train_path=dataset_folder+'/train.jsonl',\n",
        "    data_dev_path=dataset_folder+'/dev.jsonl',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    vectorize_function = sentence2vector\n",
        ")\n",
        "\n",
        "val_dataloader = sentences_dm.val_dataloader()\n",
        "\n",
        "for batch in val_dataloader:\n",
        "    X, y, z  = batch\n",
        "    print(batch)\n",
        "    print(f\"batch X shape: {X.shape}\")\n",
        "    print(f\"batch z shape: {y.shape}\")\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0P4wyI__EYdu"
      },
      "source": [
        "Create the MLP classifier class "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmfsdUysWYNl"
      },
      "source": [
        "class SentencesClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, n_features: int, n_hidden: int):\n",
        "        super().__init__()\n",
        "        self.lin1 = torch.nn.Linear(n_features, n_hidden)\n",
        "\n",
        "        self.output_layer = torch.nn.Linear(n_hidden, 1)\n",
        "\n",
        "        self.loss_fn = torch.nn.BCELoss()\n",
        "        \n",
        "\n",
        "    def forward(self, x: torch.Tensor, y: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:\n",
        "        \n",
        "        out = self.lin1(x)\n",
        "        out = torch.relu(out)\n",
        "    \n",
        "        logits = self.output_layer(out).squeeze(1)\n",
        "\n",
        "        out = torch.sigmoid(logits)\n",
        "\n",
        "        result = {'logits': logits, 'pred': out}\n",
        "\n",
        "        # compute loss\n",
        "        if y is not None:\n",
        "            loss = self.loss(out, y)\n",
        "            result['loss'] = loss\n",
        "\n",
        "        return result\n",
        "\n",
        "    def loss(self, pred, y):\n",
        "        return self.loss_fn(pred, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b45LYx6FZiD"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPmotA39E0Ds"
      },
      "source": [
        "Defining a trainer class to better separate our work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-D3B5zgbcKy"
      },
      "source": [
        "class Trainer():\n",
        "    def __init__(self, model, optimizer, device):\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.writer = SummaryWriter(comment=\"_first approach\")\n",
        "        self.model.train()  \n",
        "        self.model.to(self.device) \n",
        "\n",
        "    def train(self, train_dataset, eval_dataset, epochs=1):\n",
        "\n",
        "        train_loss = 0.0\n",
        "        for epoch in tqdm(range(epochs)):\n",
        "            epoch_loss = 0.0\n",
        "            len_train = 0\n",
        "            \n",
        "            self.model.train()\n",
        "            for step, sample in enumerate(train_dataset):\n",
        "                # inputs \n",
        "                inputs = sample[0].to(self.device)\n",
        "                # outputs \n",
        "                targets = sample[1].to(self.device)\n",
        "                output_distribution = self.model(inputs)\n",
        "                loss = self.model.loss(output_distribution['pred'], targets)  # compute loss\n",
        "                loss.backward()  # backpropagate the loss\n",
        "\n",
        "                self.optimizer.step()\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "                len_train += 1\n",
        "\n",
        "            avg_epoch_loss = epoch_loss / len_train\n",
        "            avg_eval_loss,avg_accuracy_loss,avg_f1_score = self.eval_metrics(eval_dataset)\n",
        "            \n",
        "            \n",
        "            self.writer.add_scalar(\"Train/loss\", avg_epoch_loss, epoch)\n",
        "            self.writer.add_scalar(\"Eval/loss\", avg_eval_loss, epoch)\n",
        "            self.writer.add_scalar(\"Eval/accuracy\", avg_accuracy_loss, epoch)\n",
        "            self.writer.add_scalar(\"Eval/F1_score\", avg_f1_score, epoch)\n",
        "\n",
        "            print('Epoch: {} avg loss = {:0.4f} avg_eval_loss = {:0.4f} avg_eval_acc = {:0.4f} avg_eval_f1 = {:0.4f}'.format(epoch, avg_epoch_loss, avg_eval_loss, avg_accuracy_loss, avg_f1_score))\n",
        "\n",
        "            train_loss += avg_epoch_loss\n",
        "            \n",
        "        torch.save(self.model.state_dict(),os.path.join(root_folder +\"/model\", 'state_{}.pt'.format(epoch)))  # save the model state\n",
        "        self.writer.flush()\n",
        "        avg_epoch_loss = train_loss / epochs\n",
        "        return avg_epoch_loss\n",
        "\n",
        "    def eval_metrics(self,eval_dataset):\n",
        "        self.model.eval()\n",
        "        epoch_val_loss = 0.0\n",
        "        len_val_train = 0\n",
        "        accuracy = 0\n",
        "        f1 = 0\n",
        "        for step, sample in enumerate(eval_dataset):\n",
        "            # inputs in the batch\n",
        "            inputs = sample[0].to(self.device)\n",
        "            # outputs in the batch\n",
        "            targets = sample[1].to(self.device)\n",
        "            output_distribution = self.model(inputs)\n",
        "            loss = self.model.loss(output_distribution['pred'], targets)  # compute loss    \n",
        "            y_pred = (output_distribution['pred']>0.5).float().cpu()\n",
        "            y_true = targets.cpu()\n",
        "            accuracy += accuracy_score(y_true, y_pred)\n",
        "            f1 += f1_score(y_true,y_pred)\n",
        "            #accuracy += ((output_distribution['pred'] > 0.5) == targets).float().mean().item() #TODO\n",
        "            epoch_val_loss += loss.item()\n",
        "            len_val_train += 1\n",
        "        \n",
        "        avg_eval_loss = epoch_val_loss / len_val_train\n",
        "        avg_accuracy_loss = accuracy / len_val_train\n",
        "        avg_f1_score = f1/len_val_train\n",
        "        return avg_eval_loss,avg_accuracy_loss,avg_f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkHNJ3n7JShv"
      },
      "source": [
        "Instanciating the classifier and tune some hyperparameters such as the number of hidden layers learning rate and number of epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbqT_pCjKUJE"
      },
      "source": [
        "sent_classifier = SentencesClassifier(\n",
        "    n_features=int(WE_LENGTH), \n",
        "    n_hidden=200 #@param {type:\"slider\", min:50, max:300, step:50}\n",
        ")\n",
        "learning_rate = 0.0391 #@param {type:\"slider\", min:0.0001, max:0.1, step:0.001}\n",
        "epochs = 100 #@param {type:\"slider\", min:50, max:300, step:10}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9BoAWaXJsR4"
      },
      "source": [
        "Defining the optimizer, we will use Stochastic gradient descent and instanciating the trainer to start the train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsbMs2tYbxTv"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "optimizer = torch.optim.SGD(sent_classifier.parameters(), lr=learning_rate)\n",
        "\n",
        "trainer = Trainer(sent_classifier, optimizer, device)\n",
        "\n",
        "train_dataloader = sentences_dm.train_dataloader()\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs\n",
        "avg_loss = trainer.train(train_dataloader,val_dataloader, epochs=epochs)\n",
        "print(avg_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRni2aJ-O_VY"
      },
      "source": [
        "#Second approach **(sequence encoding with RNN)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fL_KGqd_MkuA"
      },
      "source": [
        "Let's start by indexing each word in our vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqI-K3a5PBGA"
      },
      "source": [
        "def create_vocabulary():\n",
        "    word_index = dict()\n",
        "    vectors_store = []\n",
        "\n",
        "    # pad token, index = 0\n",
        "    vectors_store.append(torch.rand(int(WE_LENGTH)))\n",
        "    # unk token, index = 1\n",
        "    vectors_store.append(word_vectors[\"UNK\"])\n",
        "\n",
        "    # sep token, index = 2\n",
        "    if USE_SEP:\n",
        "        vectors_store.append(word_vectors[\"SEP\"])\n",
        "\n",
        "    for word, vector in word_vectors.items():\n",
        "        word_index[word] = len(vectors_store)\n",
        "        vectors_store.append(vector)\n",
        "\n",
        "    word_index = defaultdict(lambda: 1, word_index)  # default dict returns 1 (unk token) when unknown word\n",
        "    vectors_store = torch.stack(vectors_store)\n",
        "    return word_index,vectors_store"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4bKSVzQtMMa"
      },
      "source": [
        "Checking the shape of the built vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ6jMUGwfg-2"
      },
      "source": [
        "glove_embed = GloVEEmbedding()\n",
        "word_vectors = glove_embed.get_word_vectors()\n",
        "word_index,vectors_store = create_vocabulary()\n",
        "\n",
        "vocabulary_size, hidden_features = vectors_store.shape\n",
        "print(f\"Vocabulary size: {vocabulary_size}\\nHidden features: {hidden_features}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7tVTITMtyD_"
      },
      "source": [
        "Similar to sentence2vector in this function we map each word with the corresponding index of the built vocabulary\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOKnbTicgFxH"
      },
      "source": [
        "def sentence2indices(sentence: str,method: str,keyword: str) -> torch.Tensor:\n",
        "    return torch.tensor([word_index[word] for word in sentence.split(' ')], dtype=torch.long)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpyQV-lYuFA0"
      },
      "source": [
        "Customized collate function in order to pad the sentences in each batch and to return a custom tuple with X, X_length to calculate the offsets later, targets and indices to know where are the target words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDx2QWAhhYsp"
      },
      "source": [
        "def rnn_collate_fn(\n",
        "data_elements: List[Tuple[torch.Tensor, torch.Tensor, torch.Tensor]] # list of (x, y,z) pairs\n",
        ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "\n",
        "    X = [de[0] for de in data_elements]  # list of index tensors\n",
        "    \n",
        "    X = torch.nn.utils.rnn.pad_sequence(X, batch_first=True, padding_value=0)  #  shape (batch_size x max_seq_len)\n",
        "    \n",
        "    keyword_position = [de[2] for de in data_elements] # list of tuples indices where keyword is [[1st sent, 2nd sent]]\n",
        "    keyword_position = torch.tensor(keyword_position)\n",
        "    \n",
        "    y = [de[1] for de in data_elements]\n",
        "    y = torch.tensor(y)\n",
        "\n",
        "\n",
        "    return X, keyword_position, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwunMH6fWNkI"
      },
      "source": [
        "##### Training "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "my_Ku2SAvnBc"
      },
      "source": [
        "Recurrent classifier definition with a customized forward pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xI7M_zasi1ol"
      },
      "source": [
        "class SentencesRecurrentClassifier(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vectors_store: torch.Tensor,\n",
        "        n_hidden: int,\n",
        "        drop_prob: float,\n",
        "        bidir: bool,\n",
        "        n_layer_lstm: int\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        # embedding layer\n",
        "        self.embedding = torch.nn.Embedding.from_pretrained(vectors_store)\n",
        "        self.n_hidden = n_hidden\n",
        "        # recurrent layer\n",
        "        self.rnn = torch.nn.LSTM(input_size=vectors_store.size(1), hidden_size=n_hidden, num_layers=n_layer_lstm, batch_first=True, bidirectional=bidir)\n",
        "\n",
        "        # classification \n",
        "        if bidir:\n",
        "           n_hidden = n_hidden*2\n",
        "        self.lin1 = torch.nn.Linear(n_hidden, n_hidden)\n",
        "        self.linear_output = torch.nn.Linear(n_hidden, 1)\n",
        "\n",
        "        self.loss_fn = torch.nn.BCELoss()\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self, \n",
        "        X: torch.Tensor, \n",
        "        #X_length: torch.Tensor,\n",
        "        indices_keyword: torch.Tensor, \n",
        "        y: Optional[torch.Tensor] = None\n",
        "    ) -> Dict[str, torch.Tensor]:\n",
        "\n",
        "        \n",
        "        embedding_out = self.embedding(X)\n",
        "        # recurrent encoding\n",
        "        recurrent_out = self.rnn(embedding_out)[0]\n",
        "        # here we utilize the sequences length to retrieve the last token \n",
        "        # output for each sequence\n",
        "        \n",
        "        batch_size, seq_len, hidden_size = recurrent_out.shape\n",
        "\n",
        "        # we flatten the recurrent output now I have a long sequence of batch x seq_len vectors \n",
        "        flattened_out = recurrent_out.reshape(-1, hidden_size)\n",
        "        \n",
        "        # tensor of the start offsets of each element in the batch\n",
        "        sequences_offsets = torch.arange(batch_size, device=self.device) * seq_len\n",
        "        \n",
        "        summary_vectors_indices_sent1 = self.get_indices_keyword(indices_keyword, sequences_offsets,0)\n",
        "        \n",
        "        #summary_vectors_indices_end_first_sent = self.get_indices_keyword(indices_keyword, sequences_offsets,1)\n",
        "\n",
        "        summary_vectors_indices_sent2 = self.get_indices_keyword(indices_keyword, sequences_offsets,2)\n",
        "        \n",
        "\n",
        "        # we retrieve the vecttor of the corrseponding states for the keyword given for each sentence.\n",
        "          \n",
        "        summary_vectors_sent1 = flattened_out[summary_vectors_indices_sent1]\n",
        "        summary_vectors_sent2 = flattened_out[summary_vectors_indices_sent2]\n",
        "        \n",
        "        # do the difference of these two vectors yet retrieved\n",
        "        summary_vectors = summary_vectors_sent1 * summary_vectors_sent2\n",
        "        \n",
        "        # feedforward pass on the summary\n",
        "        out = self.lin1(summary_vectors)\n",
        "        out = F.leaky_relu(out)\n",
        "        \n",
        "\n",
        "        logits = self.linear_output(out).squeeze(1)\n",
        "        \n",
        "        pred = torch.sigmoid(logits)\n",
        "\n",
        "        result = {'logits': logits, 'pred': pred} \n",
        "        \n",
        "        # compute loss\n",
        "        if y is not None:\n",
        "            loss = self.loss(pred, y)\n",
        "            result['loss'] = loss\n",
        "        \n",
        "        return result\n",
        "        \n",
        "       \n",
        "    def loss(self, pred, y):\n",
        "        return self.loss_fn(pred, y)\n",
        "    '''\n",
        "    return the corresponding position of the indices of the keywords, for the sent_num passed, so the first if 0 is passed and the second if 2 is passed\n",
        "\n",
        "    '''\n",
        "    def get_indices_keyword(self,indices_keywords: Sequence[tuple], summary: Sequence[int] ,sent_num: int) -> torch.Tensor:\n",
        "        #[   0,   57,  114,  171,  228] = summary\n",
        "        #[ [ 6, 21],[ 4, 22],[ 6, 21],[ 4, 22] ...] = indices_keywords\n",
        "        tens_idx = torch.tensor([item[sent_num] for item in indices_keywords]).to(self.device)\n",
        "        return tens_idx + summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyB77pPuAkLs"
      },
      "source": [
        "Trainer class that will handle the training phase for the RNN classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzBUPzpQzic_"
      },
      "source": [
        "class TrainerRNN():\n",
        "    def __init__(self, model, optimizer, device, exp_details):\n",
        "\n",
        "        self.device = device\n",
        "        #self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience = 1)\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.writer = SummaryWriter(comment=\"_\"+exp_details)\n",
        "        self.model.train()  # we are using this model for training\n",
        "        self.model.to(self.device)  # move model to GPU if available\n",
        "\n",
        "    def train(self, train_dataset, eval_dataset, epochs: int = 1, early_stopping: bool = False, early_stopping_patience:int = 3, to_be_saved: bool =False) -> float:\n",
        "\n",
        "        train_loss = 0.0\n",
        "        eval_loss = 0.0\n",
        "        eval_acc = 0.0\n",
        "        eval_f1 = 0.0\n",
        "        valid_history = []\n",
        "        patience_counter = 0\n",
        "        best_avg_acc = 0.679\n",
        "        for epoch in tqdm(range(epochs)):\n",
        "            epoch_loss = 0.0\n",
        "            len_train = 0\n",
        "            \n",
        "            self.model.train()\n",
        "\n",
        "            # each element (sample) in train_dataset is a batch\n",
        "            for sample in train_dataset:\n",
        "\n",
        "\n",
        "                # inputs in the batch\n",
        "                inputs = sample[0].to(self.device)\n",
        "                # indices of keywords\n",
        "                idx_start = sample[1].to(self.device)\n",
        "\n",
        "                # outputs in the batch\n",
        "                targets = sample[2].to(self.device)\n",
        "                \n",
        "                output_distribution = self.model(inputs, idx_start, targets)\n",
        "\n",
        "                loss = output_distribution['loss']\n",
        "                \n",
        "                loss.backward()  #  backpropagate the loss\n",
        "                # updates the parameters\n",
        "                #Clips gradient norm of an iterable of parameters.\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), CLIP_GRAD)\n",
        "                self.optimizer.step()\n",
        "                self.optimizer.zero_grad()\n",
        "                \n",
        "                epoch_loss += loss.item()\n",
        "                len_train += 1\n",
        "                \n",
        "            avg_epoch_loss = epoch_loss / len_train\n",
        "\n",
        "            avg_eval_loss,avg_accuracy_loss,avg_f1_score, avg_recall_score = self.eval_metrics(eval_dataset)\n",
        "            #self.model.train()\n",
        "            #self.scheduler.step(avg_eval_loss)\n",
        "            valid_history.append(avg_accuracy_loss)\n",
        "\n",
        "            if early_stopping and epoch > 0:\n",
        "                if valid_history[-1] < valid_history[-2]:\n",
        "                    if patience_counter >= early_stopping_patience:\n",
        "                        print('Early stop.')\n",
        "                        break\n",
        "                    else:\n",
        "                        print('-- Patience.\\n')\n",
        "                        patience_counter += 1\n",
        "            \n",
        "            self.writer.add_scalar(\"Train/loss\", avg_epoch_loss, epoch)\n",
        "            self.writer.add_scalar(\"Eval/loss\", avg_eval_loss, epoch)\n",
        "            self.writer.add_scalar(\"Eval/accuracy\", avg_accuracy_loss, epoch)\n",
        "            self.writer.add_scalar(\"Eval/F1_score\", avg_f1_score, epoch)\n",
        "            self.writer.add_scalar(\"Eval/recall_score\", avg_recall_score, epoch)\n",
        "            print('Epoch: {} avg loss = {:0.4f} eval loss = {:0.4f} ACC = {:0.4f} F1 = {:0.4f} RECALL = {:0.4f}'.format(epoch, avg_epoch_loss, avg_eval_loss, avg_accuracy_loss,avg_f1_score,avg_recall_score))\n",
        "            \n",
        "            train_loss += avg_epoch_loss\n",
        "            eval_loss += avg_eval_loss\n",
        "            eval_acc += avg_accuracy_loss\n",
        "            eval_f1 += avg_f1_score\n",
        "\n",
        "\n",
        "            self.writer.flush()\n",
        "    \n",
        "            if to_be_saved:\n",
        "                if avg_accuracy_loss > best_avg_acc and epoch>3:\n",
        "                    torch.save(self.model.state_dict(), root_folder+'/model/'+exp_details+'_epoch_{}_acc_{:0.4f}.pt'.format(epoch, avg_accuracy_loss)) # save the model state\n",
        "                    best_avg_acc = avg_accuracy_loss        \n",
        "        #torch.save(self.model.state_dict(),root_folder+'/model/'+exp_details+'_epoch_{}_acc_{:0.4f}.pt'.format(epoch, avg_accuracy_loss)) # save the model state\n",
        "                    \n",
        "        #epoch+1 because if early stopping is true, the metrics are not sure to be arrived at number of epochs\n",
        "\n",
        "        return [train_loss / (epoch+1), eval_loss / (epoch+1), eval_acc / (epoch+1), eval_f1 / (epoch+1)]\n",
        "\n",
        "    '''\n",
        "        returns the metrics of the current epoch\n",
        "    '''\n",
        "    def eval_metrics(self,eval_dataset):\n",
        "        self.model.eval()\n",
        "        epoch_val_loss = 0.0\n",
        "        len_val_train = 0\n",
        "        accuracy = 0.0\n",
        "        recall = 0.0\n",
        "        f1 = 0.0\n",
        "        for sample in eval_dataset:\n",
        "            # inputs in the batch\n",
        "            inputs = sample[0].to(self.device)\n",
        "            idx_start = sample[1].to(self.device)\n",
        "\n",
        "            # outputs in the batch\n",
        "            targets = sample[2].to(self.device)\n",
        "\n",
        "\n",
        "            output_distribution = self.model(inputs, idx_start)\n",
        "            loss = self.model.loss(output_distribution['pred'], targets)  # compute loss  \n",
        "            y_pred = (output_distribution['pred']>0.5).float().cpu()\n",
        "            y_true = targets.cpu()\n",
        "            \n",
        "            accuracy += accuracy_score(y_true, y_pred)\n",
        "            f1 += f1_score(y_true,y_pred)\n",
        "            recall += recall_score(y_true, y_pred)\n",
        "            epoch_val_loss += loss.item()\n",
        "            \n",
        "            len_val_train += 1\n",
        "\n",
        "        avg_eval_loss = epoch_val_loss / len_val_train\n",
        "        avg_accuracy_loss = accuracy / len_val_train\n",
        "        avg_f1_score = f1/len_val_train\n",
        "        avg_recall_score = recall/len_val_train\n",
        "\n",
        "        return avg_eval_loss, avg_accuracy_loss, avg_f1_score, avg_recall_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ysl5lG_pxhY9"
      },
      "source": [
        "Loading of the handler for the dataset and choose of the batch size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeaTHJCvvqCh"
      },
      "source": [
        "BATCH_SIZE = 40 #@param {type:\"slider\", min:8, max:64, step:4}\n",
        "\n",
        "sentences_rnn_dm = SentencesDataModule(\n",
        "    data_train_path=dataset_folder+'/train.jsonl',\n",
        "    data_dev_path=dataset_folder+'/dev.jsonl',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn = rnn_collate_fn,\n",
        "    vectorize_function = sentence2indices\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o9d52ZOxdiI"
      },
      "source": [
        "Hyperparameter setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j51GSfZSvyTv"
      },
      "source": [
        "#@title Setup of Hyper-parameters{ run: \"auto\" }\n",
        "\n",
        "n_hidden=82 #@param {type:\"slider\", min:50, max:300, step:16}\n",
        "drop_prob=0.5 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "bidir = True #@param [\"True\", \"False\"] {type:\"raw\"}\n",
        "learning_rate = 0.0001 #@param {type:\"slider\", min:0.00001, max:0.001, step:0.00001}\n",
        "epochs = 25 #@param {type:\"slider\", min:10, max:100, step:10}\n",
        "n_layer_lstm = 2 #@param {type:\"slider\", min:1, max:4, step:1}\n",
        "CLIP_GRAD = 1 #@param {type:\"slider\", min:1, max:10, step:1}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxReoMvS70jb"
      },
      "source": [
        "Start the training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmaXIR2DjtCg"
      },
      "source": [
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "sentences_recurrent_classifier = SentencesRecurrentClassifier(vectors_store, n_hidden=n_hidden,drop_prob=drop_prob, bidir = bidir, n_layer_lstm = n_layer_lstm)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "#define the optimizer\n",
        "optimizer = torch.optim.Adam(sentences_recurrent_classifier.parameters(), lr=learning_rate)\n",
        "\n",
        "# string to indetify the model once saved or in the graphs\n",
        "exp_details=\"diff_leakyrelu_\" + str(drop_prob) + \"drop_\"+str(n_hidden) +\"hidden_\"+str(learning_rate) +\"lr_\" + str(BATCH_SIZE) +\"batch_\" + str(n_layer_lstm) +\"lstmLayer_\" + str(CLIP_GRAD) +\"clipGrad\"\n",
        "\n",
        "trainer = TrainerRNN(sentences_recurrent_classifier, optimizer, device, exp_details, )\n",
        "\n",
        "#loading of the datasets\n",
        "train_dataloader = sentences_rnn_dm.train_dataloader()\n",
        "val_dataloader = sentences_rnn_dm.val_dataloader()\n",
        "\n",
        "avg_train_loss,  avg_eval_loss, avg_accuracy_loss, avg_f1_score = trainer.train(train_dataloader, val_dataloader, epochs=epochs, early_stopping=False, early_stopping_patience=6, to_be_saved = True)\n",
        "print(\" avg_train_loss={}\\n avg_eval_loss={}\\n avg_acc_loss={}\\n avg_f1_loss={}\\n\".format(avg_train_loss , avg_eval_loss, avg_accuracy_loss, avg_f1_score))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ga3YZRyF737b"
      },
      "source": [
        "See some nice graphs of the finished training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFKYSAHv7DKl"
      },
      "source": [
        "try:\n",
        "    %reload_ext tensorboard\n",
        "except:\n",
        "    %load_ext tensorboard\n",
        "%tensorboard --logdir=runs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}