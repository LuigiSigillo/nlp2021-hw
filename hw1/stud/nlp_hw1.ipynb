{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp-hw1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "8b45LYx6FZiD"
      ],
      "authorship_tag": "ABX9TyOcsAmdANIGFTFm/Mc3dehn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9671b5b818b743f7b986464de45569b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_86f6ef23fa18461da5d60084776239da",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fb7c3ce893a044bd86b7305fa57c9b6a",
              "IPY_MODEL_58e996a4d6d34406984358b61733ff76"
            ]
          }
        },
        "86f6ef23fa18461da5d60084776239da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fb7c3ce893a044bd86b7305fa57c9b6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1522244675e1477990116bd358d78769",
            "_dom_classes": [],
            "description": " 99%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 100000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 98739,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_93581ea928ff416f86fa14eb81e68646"
          }
        },
        "58e996a4d6d34406984358b61733ff76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ca4d463b24774d9c9396288bbfc47d06",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 98739/100000 [00:20&lt;00:00, 27332.26it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8e451b30d3bd409c97233797cc55691e"
          }
        },
        "1522244675e1477990116bd358d78769": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "93581ea928ff416f86fa14eb81e68646": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ca4d463b24774d9c9396288bbfc47d06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8e451b30d3bd409c97233797cc55691e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6deb972fdcc1437595a0906a9910a248": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_00fc3e5c674741ceaeb5624f41e17751",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_664d64f7829043c2a17ecd2ca3a76eee",
              "IPY_MODEL_f15df09db02d404aa0216eb1d4350b8a"
            ]
          }
        },
        "00fc3e5c674741ceaeb5624f41e17751": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "664d64f7829043c2a17ecd2ca3a76eee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_96fd5477430c4b6c89ec150312736b23",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 150,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 150,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c861684fc1cc482e9bf2adf9bd779d55"
          }
        },
        "f15df09db02d404aa0216eb1d4350b8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5a0dc0f94d784b6f89bfe523501edb83",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 150/150 [06:27&lt;00:00,  2.58s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b9e7789ccb6f404cb9fef01a77883f64"
          }
        },
        "96fd5477430c4b6c89ec150312736b23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c861684fc1cc482e9bf2adf9bd779d55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5a0dc0f94d784b6f89bfe523501edb83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b9e7789ccb6f404cb9fef01a77883f64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuigiSigillo/nlp2021-hw/blob/master/hw1/stud/nlp_hw1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0y_g6ETA_lf3"
      },
      "source": [
        "# imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2n5xn1F5kvq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5e7f029-663f-4089-dee5-def665dabb3d"
      },
      "source": [
        "from google.colab import drive\n",
        "# general\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from collections import Counter, defaultdict\n",
        "from tqdm.notebook import tqdm\n",
        "from typing import *\n",
        "\n",
        "# torch\n",
        "import torch\n",
        "import json\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import SGD\n",
        "\n",
        "# NLTK\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "root_folder = '/content/drive/My Drive/NLP/nlp2021-hw1'\n",
        "dataset_folder = os.path.join(root_folder,'data')\n",
        "print(dataset_folder)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/NLP/nlp2021-hw1/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbAnAo7pMvbq"
      },
      "source": [
        "#! wget http://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
        "#! unzip -d data/glove.6B\n",
        "#! cd '/content/drive/My Drive/NLP/nlp2021-hw1'\n",
        "#!unzip '/content/drive/My Drive/NLP/nlp2021-hw1/glove.6B.zip'\n",
        "# !mv glove.6B.200d.txt '/content/drive/My Drive/NLP/nlp2021-hw1/model'\n",
        "# !ls"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anwpXc08XRKf"
      },
      "source": [
        " load the actual word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0,
          "referenced_widgets": [
            "9671b5b818b743f7b986464de45569b5",
            "86f6ef23fa18461da5d60084776239da",
            "fb7c3ce893a044bd86b7305fa57c9b6a",
            "58e996a4d6d34406984358b61733ff76",
            "1522244675e1477990116bd358d78769",
            "93581ea928ff416f86fa14eb81e68646",
            "ca4d463b24774d9c9396288bbfc47d06",
            "8e451b30d3bd409c97233797cc55691e"
          ]
        },
        "id": "Te3_zheKQQDe",
        "outputId": "2ab63311-40b8-49f3-90e7-a1dc190ca981"
      },
      "source": [
        "word_vectors = dict()\n",
        "words_limit = 100_000\n",
        "with open('/content/drive/My Drive/NLP/nlp2021-hw1/model/glove.6B.100d.txt') as f:\n",
        "\n",
        "    next(f)  # skip header\n",
        "\n",
        "    for i, line in tqdm(enumerate(f), total=words_limit):\n",
        "\n",
        "        if i == words_limit:\n",
        "            break\n",
        "\n",
        "        word, *vector = line.strip().split(' ')\n",
        "        vector = torch.tensor([float(c) for c in vector])\n",
        "        \n",
        "        word_vectors[word] = vector\n",
        "# word_vectors[\"UNK\"] = np.mean(np.array(list(word_vectors.values()), dtype=np.float64), axis=0)\n",
        "word_vectors[\"UNK\"] = torch.tensor(np.random.random(100),dtype=torch.float)\n",
        "word_vectors[\"<SEP>\"] = torch.tensor(np.random.random(100),dtype=torch.float)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9671b5b818b743f7b986464de45569b5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKed-2p_XsMH"
      },
      "source": [
        "word-embedding-powered function $\\phi$.  just converts any review to a vector by **averaging the embeddings corresponding to each word in it**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpsIPHyUV6Lt"
      },
      "source": [
        "def phrase2vector(phrase: str, method: str, keyword: str) -> Optional[torch.Tensor]:\n",
        "    #phrases_word_vector = [word_vectors[w] if w in word_vectors else word_vectors['UNK'] for w in phrase.split(' ')]\n",
        "    # if len(phrases_word_vector) == 0:\n",
        "    #     return None\n",
        "    phrases_word_vector = []\n",
        "    for w in phrase.split(' '):\n",
        "        coeff = 1\n",
        "        if w in word_vectors:\n",
        "            if w == keyword:\n",
        "                coeff = 1.5\n",
        "            phrases_word_vector.append(word_vectors[w]*coeff)\n",
        "        else:\n",
        "            phrases_word_vector.append(word_vectors['UNK'])\n",
        "    \n",
        "    if len(phrases_word_vector) == 0:\n",
        "        return None\n",
        "\n",
        "    phrases_word_vector = torch.stack(phrases_word_vector)  # tensor shape: (#words X #features)\n",
        "    if method==\"avg\":\n",
        "        return torch.mean(phrases_word_vector, dim=0)\n",
        "    elif method==\"sum\":\n",
        "        return torch.sum(phrases_word_vector, dim=0)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ti80ryeyXUXT"
      },
      "source": [
        "class SentencesDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, dataset_path: str, phrase2vector):\n",
        "        self.data_store = []\n",
        "        self.init_structures(dataset_path, phrase2vector)\n",
        "\n",
        "    def init_structures(self, dataset_path: str, phrase2vector) -> None:\n",
        "\n",
        "        with open(dataset_path) as f:\n",
        "            for json_string in f:\n",
        "                single_json = json.loads(json_string)\n",
        "                keyword = single_json['sentence1'][int(single_json['start1']):int(single_json['end1'])]\n",
        "                sentence =  self.remove_stopwords(single_json['sentence1']) + \" <SEP> \" + self.remove_stopwords(single_json['sentence2'])\n",
        "                ground_t = np.float32(1) if single_json['label'] =='True' else np.float32(0)\n",
        "                vector = phrase2vector(sentence,\"avg\",keyword)\n",
        "                if vector is None:\n",
        "                    continue\n",
        "                self.data_store.append((vector,ground_t))\n",
        "                \n",
        "\n",
        "\n",
        "\n",
        "    def remove_stopwords(self,sent):\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        word_tokens = word_tokenize(sent)\n",
        "        filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
        "        filtered_sentence = []\n",
        "\n",
        "        for w in word_tokens:\n",
        "            if w not in stop_words:\n",
        "                filtered_sentence.append(w)\n",
        "        \n",
        "        return \" \".join(filtered_sentence)\n",
        "    \n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data_store)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
        "        return self.data_store[idx]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8K2TtSOXY13"
      },
      "source": [
        "class SentencesDataModule(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self, \n",
        "        data_train_path: str,\n",
        "        data_dev_path: str,\n",
        "        batch_size: int,\n",
        "        collate_fn=None\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.data_train_path = data_train_path\n",
        "        self.data_dev_path = data_dev_path\n",
        "        self.batch_size = batch_size\n",
        "        self.collate_fn = collate_fn\n",
        "\n",
        "        self.train_dataset = None\n",
        "        self.validation_dataset = None\n",
        "        self.test_dataset = None\n",
        "\n",
        "    def setup(self, stage: Optional[str] = None) -> None:\n",
        "        self.train_dataset = SentencesDataset(self.data_train_path, phrase2vector)\n",
        "        self.validation_dataset = SentencesDataset(self.data_dev_path, phrase2vector)\n",
        "\n",
        "    def train_dataloader(self, *args, **kwargs) -> DataLoader:\n",
        "        return DataLoader(self.train_dataset, batch_size=self.batch_size)\n",
        "\n",
        "    def val_dataloader(self, *args, **kwargs) -> Union[DataLoader, List[DataLoader]]:\n",
        "        return DataLoader(self.validation_dataset, batch_size=self.batch_size)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwuHz1kdXryl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ed4dcf2-6121-4124-f200-177a1c4ade1f"
      },
      "source": [
        "sentences_dm = SentencesDataModule(\n",
        "    data_train_path=dataset_folder+'/train.jsonl',\n",
        "    data_dev_path=dataset_folder+'/dev.jsonl',\n",
        "    batch_size=32,\n",
        ")\n",
        "sentences_dm.setup('fit')\n",
        "val_dataloader = sentences_dm.val_dataloader()\n",
        "# print(word_vectors['test'])\n",
        "\n",
        "for batch in val_dataloader:\n",
        "    X, y = batch\n",
        "    print(batch)\n",
        "    print(f\"batch X shape: {X.shape}\")\n",
        "    print(f\"batch z shape: {y.shape}\")\n",
        "    \n",
        "    \n",
        "    break"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[tensor([[ 2.0530e-01,  5.6846e-02,  3.1384e-01,  ..., -1.3134e-01,\n",
            "          2.9982e-01,  2.7490e-01],\n",
            "        [ 2.5217e-01, -2.1304e-01,  3.3004e-01,  ..., -8.6169e-02,\n",
            "          2.9549e-01,  1.1427e-01],\n",
            "        [ 1.1557e-01,  6.5798e-02,  2.4409e-01,  ..., -1.5208e-01,\n",
            "          4.5113e-01,  1.2485e-01],\n",
            "        ...,\n",
            "        [ 9.8717e-02,  2.7772e-01,  2.7141e-01,  ..., -9.9638e-02,\n",
            "          2.8112e-01,  4.7557e-01],\n",
            "        [-2.1115e-02,  1.1390e-01,  2.2829e-01,  ..., -1.8001e-01,\n",
            "          5.7082e-01,  1.5087e-01],\n",
            "        [-4.5604e-02,  5.4063e-02,  2.0879e-01,  ..., -1.4570e-01,\n",
            "          5.3284e-01, -1.4090e-04]]), tensor([0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0.,\n",
            "        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.])]\n",
            "batch X shape: torch.Size([32, 100])\n",
            "batch z shape: torch.Size([32])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b45LYx6FZiD"
      },
      "source": [
        "# Non so se training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmfsdUysWYNl"
      },
      "source": [
        "class SentencesClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, n_features: int, n_hidden: int):\n",
        "        super().__init__()\n",
        "        # classification function\n",
        "        self.lin1 = torch.nn.Linear(n_features, n_hidden)\n",
        "        self.output_layer = torch.nn.Linear(n_hidden, 1)\n",
        "        \n",
        "        # criterion\n",
        "        self.loss_fn = torch.nn.BCELoss()\n",
        "        \n",
        "\n",
        "    def forward(self, x: torch.Tensor, y: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:\n",
        "        # actual forward\n",
        "        out = self.lin1(x)\n",
        "        out = torch.relu(out)\n",
        "        # compute logits (which are simply the out variable) and the actual probability distribution (pred, as it is the predicted distribution)\n",
        "    \n",
        "        logits = self.output_layer(out).squeeze(1)\n",
        "\n",
        "        out = torch.sigmoid(logits)\n",
        "\n",
        "        result = {'logits': logits, 'pred': out}\n",
        "\n",
        "        # compute loss\n",
        "        if y is not None:\n",
        "            # torch optimizes its computation internally and takes as input the logits instead\n",
        "            loss = self.loss(out, y)\n",
        "            result['loss'] = loss\n",
        "\n",
        "        return result\n",
        "\n",
        "    def loss(self, pred, y):\n",
        "        return self.loss_fn(pred, y)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-D3B5zgbcKy"
      },
      "source": [
        "class Trainer():\n",
        "    def __init__(self, model, optimizer, device):\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        # starts requires_grad for all layers\n",
        "        self.model.train()  # we are using this model for training (some layers have different behaviours in train and eval mode)\n",
        "        self.model.to(self.device)  # move model to GPU if available\n",
        "\n",
        "    def train(self, train_dataset, eval_dataset, epochs=1):\n",
        "\n",
        "        train_loss = 0.0\n",
        "        for epoch in tqdm(range(epochs)):\n",
        "            epoch_loss = 0.0\n",
        "            len_train = 0\n",
        "            epoch_val_loss = 0.0\n",
        "            len_val_train = 0\n",
        "            accuracy = 0\n",
        "            self.model.train()\n",
        "            # each element (sample) in train_dataset is a batch\n",
        "            for step, sample in enumerate(train_dataset):\n",
        "                # inputs in the batch\n",
        "                inputs = sample[0].to(self.device)\n",
        "                # outputs in the batch\n",
        "                targets = sample[1].to(self.device)\n",
        "                output_distribution = self.model(inputs)\n",
        "                loss = self.model.loss(output_distribution['pred'], targets)  # compute loss\n",
        "                # calculates the gradient and accumulates\n",
        "                loss.backward()  # we backpropagate the loss\n",
        "                # updates the parameters\n",
        "                self.optimizer.step()\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "                len_train += 1\n",
        "            \n",
        "            self.model.eval()\n",
        "            for step, sample in enumerate(eval_dataset):\n",
        "                # inputs in the batch\n",
        "                inputs = sample[0].to(self.device)\n",
        "                # outputs in the batch\n",
        "                targets = sample[1].to(self.device)\n",
        "                output_distribution = self.model(inputs)\n",
        "                loss = self.model.loss(output_distribution['pred'], targets)  # compute loss    \n",
        "                \n",
        "                accuracy += ((output_distribution['pred'] > 0.5) == targets).float().mean().item() #TODO\n",
        "                epoch_val_loss += loss.item()\n",
        "                len_val_train += 1\n",
        "            \n",
        "            avg_epoch_loss = epoch_loss / len_train\n",
        "            avg_eval_loss = epoch_val_loss / len_val_train\n",
        "            avg_accuracy_loss = accuracy / len_val_train\n",
        "            print('Epoch: {} avg loss = {:0.4f} eval loss = {:0.4f} ACC = {:0.4f}'.format(epoch, avg_epoch_loss, avg_eval_loss, avg_accuracy_loss))\n",
        "\n",
        "            train_loss += avg_epoch_loss\n",
        "            # torch.save(self.model.state_dict(),\n",
        "            #            os.path.join(output_folder, 'state_{}.pt'.format(epoch)))  # save the model state\n",
        "\n",
        "        avg_epoch_loss = train_loss / epochs\n",
        "        return avg_epoch_loss"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbqT_pCjKUJE"
      },
      "source": [
        "sent_classifier = SentencesClassifier(\n",
        "    n_features=100, \n",
        "    n_hidden=128\n",
        ")"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsbMs2tYbxTv"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "optimizer = torch.optim.SGD(sent_classifier.parameters(), lr=0.02)\n",
        "trainer = Trainer(sent_classifier, optimizer, device)\n",
        "train_dataloader = sentences_dm.train_dataloader()\n",
        "avg_loss = trainer.train(train_dataloader,val_dataloader, epochs=150)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cijah8M5QXXN"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxGcNR-cQXjP",
        "outputId": "c01d4962-f9c2-43f4-edfd-85af99686dc9"
      },
      "source": [
        "def predict(model, phrase2vector, review: str):\n",
        "    phrase_vector = phrase2vector(review,\"avg\",\"mouse\").to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    forward_out = model(phrase_vector.unsqueeze(0))  # add a dimension to create a one-item batch\n",
        "    print(f\"# Sentences: {review}\")\n",
        "    for i,prob in enumerate(forward_out[\"pred\"]):\n",
        "        print(\"\\n {}\".format( prob) )\n",
        "predict(sent_classifier, phrase2vector, \"The cat eats the mouse <SEP> Use the mouse to click on the button\")\n",
        "predict(sent_classifier, phrase2vector, \"The cat eats the mouse <SEP> The mouse escaped from the predator\")\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# Sentences: The cat eats the mouse <SEP> Use the mouse to click on the button\n",
            "\n",
            " 0.9217332005500793\n",
            "# Sentences: The cat eats the mouse <SEP> The mouse escaped from the predator\n",
            "\n",
            " 0.9201757311820984\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRni2aJ-O_VY"
      },
      "source": [
        "# RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqI-K3a5PBGA"
      },
      "source": [
        "word_index = dict()\n",
        "vectors_store = []\n",
        "\n",
        "# pad token, index = 0\n",
        "vectors_store.append(torch.rand(100))\n",
        "\n",
        "# unk token, index = 1\n",
        "vectors_store.append(torch.rand(100))\n",
        "\n",
        "for word, vector in word_vectors.items():\n",
        "    word_index[word] = len(vectors_store)\n",
        "    vectors_store.append(vector)\n",
        "\n",
        "word_index = defaultdict(lambda: 1, word_index)  # default dict returns 1 (unk token) when unknown word\n",
        "vectors_store = torch.stack(vectors_store)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQ6jMUGwfg-2",
        "outputId": "4ac92aed-b330-4dc2-a53c-225944e44c0e"
      },
      "source": [
        "vocabulary_size, hidden_features = vectors_store.shape\n",
        "print(f\"Vocabulary size: {vocabulary_size}\")\n",
        "print(f\"Hidden features: {hidden_features}\")\n",
        "word_index['pezzo']  # let's see if the word_index gives to us the unk index (1)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size: 100004\n",
            "Hidden features: 100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOKnbTicgFxH"
      },
      "source": [
        "def review2indices(review: str,method: str,keyword: str) -> torch.Tensor:\n",
        "    return torch.tensor([word_index[word] for word in review.split(' ')], dtype=torch.long)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDx2QWAhhYsp"
      },
      "source": [
        "def rnn_collate_fn(\n",
        "    data_elements: List[Tuple[torch.Tensor, torch.Tensor]] # list of (x, y) pairs\n",
        ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "\n",
        "    X = [de[0] for de in data_elements]  # list of index tensors\n",
        "\n",
        "    # to implement the many-to-one strategy\n",
        "    X_lengths = torch.tensor([x.size(0) for x in X], dtype=torch.long)\n",
        "\n",
        "    X = torch.nn.utils.rnn.pad_sequence(X, batch_first=True, padding_value=0)  #  shape (batch_size x max_seq_len)\n",
        "\n",
        "    y = [de[1] for de in data_elements]\n",
        "    y = torch.tensor(y)\n",
        "\n",
        "    return X, X_lengths, y"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NJyDXpahn7P"
      },
      "source": [
        "class AmazonReviewsRNNDataModule(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self, \n",
        "        data_train_path: str,\n",
        "        data_dev_path: str,\n",
        "        data_test_path: str,\n",
        "        batch_size: int,\n",
        "        collate_fn=None\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.data_train_path = data_train_path\n",
        "        self.data_dev_path = data_dev_path\n",
        "        self.data_test_path = data_test_path\n",
        "        self.batch_size = batch_size\n",
        "        self.collate_fn = collate_fn\n",
        "\n",
        "        self.train_dataset = None\n",
        "        self.validation_dataset = None\n",
        "        self.test_dataset = None\n",
        "\n",
        "    def setup(self, stage: Optional[str] = None) -> None:\n",
        "        if stage == 'fit':\n",
        "            self.train_dataset = SentencesDataset(self.data_train_path, review2indices)\n",
        "            self.validation_dataset = SentencesDataset(self.data_dev_path, review2indices)\n",
        "        elif stage == 'test':\n",
        "            self.test_dataset = SentencesDataset(self.data_test_path, review2indices)\n",
        "\n",
        "    def train_dataloader(self, *args, **kwargs) -> DataLoader:\n",
        "        return DataLoader(self.train_dataset, batch_size=self.batch_size, collate_fn=rnn_collate_fn)\n",
        "\n",
        "    def val_dataloader(self, *args, **kwargs) -> Union[DataLoader, List[DataLoader]]:\n",
        "        return DataLoader(self.validation_dataset, batch_size=self.batch_size, collate_fn=rnn_collate_fn)\n",
        "\n",
        "    def test_dataloader(self, *args, **kwargs) -> Union[DataLoader, List[DataLoader]]:\n",
        "        return DataLoader(self.test_dataset, batch_size=self.batch_size, collate_fn=rnn_collate_fn)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejkjswCVhtpz"
      },
      "source": [
        "amazon_review_rnn_dm = AmazonReviewsRNNDataModule(\n",
        "    data_train_path=dataset_folder+'/train.jsonl',\n",
        "    data_dev_path=dataset_folder+'/dev.jsonl',\n",
        "    data_test_path='data/dataset.test.tsv',\n",
        "    batch_size=32,\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7ODza0Zh4oH",
        "outputId": "51db555a-eabb-4835-99a7-41a0fe5e9a14"
      },
      "source": [
        "amazon_review_rnn_dm.setup('fit')\n",
        "\n",
        "for batch in amazon_review_rnn_dm.val_dataloader():\n",
        "    batch_X, batch_X_lengths, batch_y = batch\n",
        "    print(batch_X)\n",
        "    print(batch_X.shape)\n",
        "    print(batch_y.shape)\n",
        "    break"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[    1,  8195,   954,  ...,  9580,     1,     3],\n",
            "        [    1,  8195,   954,  ...,     0,     0,     0],\n",
            "        [    1, 19372,   249,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [    1,   360,  2604,  ...,     0,     0,     0],\n",
            "        [    1,   750,  5518,  ...,     0,     0,     0],\n",
            "        [    1,   750,  5518,  ...,     0,     0,     0]])\n",
            "torch.Size([32, 44])\n",
            "torch.Size([32])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xI7M_zasi1ol"
      },
      "source": [
        "class AmazonReviewRecurrentClassifier(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vectors_store: torch.Tensor,\n",
        "        n_hidden: int\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        # embedding layer\n",
        "        self.embedding = torch.nn.Embedding.from_pretrained(vectors_store)\n",
        "\n",
        "        # recurrent layer\n",
        "        self.rnn = torch.nn.LSTM(input_size=vectors_store.size(1), hidden_size=n_hidden, num_layers=1, batch_first=True)\n",
        "\n",
        "        # classification head\n",
        "        self.lin1 = torch.nn.Linear(n_hidden, n_hidden)\n",
        "        self.lin2 = torch.nn.Linear(n_hidden, 1)\n",
        "\n",
        "        # criterion\n",
        "        self.loss_fn = torch.nn.BCELoss()\n",
        "        self.device = 'cuda'\n",
        "\n",
        "    def forward(\n",
        "        self, \n",
        "        X: torch.Tensor, \n",
        "        X_length: torch.Tensor, \n",
        "        y: Optional[torch.Tensor] = None\n",
        "    ) -> Dict[str, torch.Tensor]:\n",
        "\n",
        "        # embedding words from indices\n",
        "        embedding_out = self.embedding(X)\n",
        "\n",
        "        # recurrent encoding\n",
        "        recurrent_out = self.rnn(embedding_out)[0]\n",
        "        \n",
        "        # here we utilize the sequences length to retrieve the last token \n",
        "        # output for each sequence\n",
        "        batch_size, seq_len, hidden_size = recurrent_out.shape\n",
        "\n",
        "        # we flatten the recurrent output\n",
        "        # now I have a long sequence of batch x seq_len vectors \n",
        "        flattened_out = recurrent_out.reshape(-1, hidden_size)\n",
        "        \n",
        "        # and we use a simple trick to compute a tensor of the indices \n",
        "        # of the last token in each batch element\n",
        "        last_word_relative_indices = X_length - 1\n",
        "        # tensor of the start offsets of each element in the batch\n",
        "        sequences_offsets = torch.arange(batch_size, device=self.device) * seq_len\n",
        "        # e.g. (0, 5, 10, 15, ) + ( 3, 2, 1, 4 ) = ( 3, 7, 11, 19 )\n",
        "        summary_vectors_indices = sequences_offsets + last_word_relative_indices\n",
        "\n",
        "        # finaly we retrieve the vectors that should summarize every review.\n",
        "        # (i.e. the last token in the sequence)\n",
        "        summary_vectors = flattened_out[summary_vectors_indices]\n",
        "\n",
        "        # now we can classify the reviews with a feedforward pass on the summary\n",
        "        # vectors\n",
        "        out = self.lin1(summary_vectors)\n",
        "        out = torch.relu(out)\n",
        "        out = self.lin2(out).squeeze(1)\n",
        "\n",
        "        # compute logits (which are simply the out variable) and the actual probability distribution (pred, as it is the predicted distribution)\n",
        "        logits = out\n",
        "        pred = torch.softmax(logits, dim=-1)\n",
        "\n",
        "        result = {'logits': logits, 'pred': pred}\n",
        "\n",
        "        # compute loss\n",
        "        if y is not None:\n",
        "            # while mathematically the CrossEntropyLoss takes as input the probability distributions,\n",
        "            # torch optimizes its computation internally and takes as input the logits instead\n",
        "            loss = self.loss(logits, y)\n",
        "            result['loss'] = loss\n",
        "\n",
        "        return result\n",
        "\n",
        "    def loss(self, pred, y):\n",
        "        return self.loss_fn(pred, y)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzBUPzpQzic_"
      },
      "source": [
        "class Trainer():\n",
        "    def __init__(self, model, optimizer, device):\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        # starts requires_grad for all layers\n",
        "        self.model.train()  # we are using this model for training (some layers have different behaviours in train and eval mode)\n",
        "        self.model.to(self.device)  # move model to GPU if available\n",
        "\n",
        "    def train(self, train_dataset, eval_dataset, epochs=1):\n",
        "\n",
        "        train_loss = 0.0\n",
        "        for epoch in tqdm(range(epochs)):\n",
        "            epoch_loss = 0.0\n",
        "            len_train = 0\n",
        "            epoch_val_loss = 0.0\n",
        "            len_val_train = 0\n",
        "            accuracy = 0\n",
        "            self.model.train()\n",
        "            # each element (sample) in train_dataset is a batch\n",
        "            for step, sample in enumerate(train_dataset):\n",
        "                # inputs in the batch\n",
        "                inputs = sample[0].to(self.device)\n",
        "                x_lenghts = sample[1].to(self.device)\n",
        "                # outputs in the batch\n",
        "                targets = sample[2].to(self.device)\n",
        "                output_distribution = self.model(inputs, x_lenghts)\n",
        "                loss = self.model.loss(output_distribution['pred'], targets)  # compute loss\n",
        "                # calculates the gradient and accumulates\n",
        "                loss.backward()  # we backpropagate the loss\n",
        "                # updates the parameters\n",
        "                self.optimizer.step()\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "                len_train += 1\n",
        "                \n",
        "            self.model.eval()\n",
        "            for step, sample in enumerate(eval_dataset):\n",
        "                # inputs in the batch\n",
        "                inputs = sample[0].to(self.device)\n",
        "                x_lenghts = sample[1].to(self.device)\n",
        "                # outputs in the batch\n",
        "                targets = sample[2].to(self.device)\n",
        "                output_distribution = self.model(inputs, x_lenghts)\n",
        "                loss = self.model.loss(output_distribution['pred'], targets)  # compute loss    \n",
        "                \n",
        "                accuracy += ((output_distribution['pred'] > 0.5) == targets).float().mean().item() #TODO\n",
        "                epoch_val_loss += loss.item()\n",
        "                len_val_train += 1\n",
        "            \n",
        "            avg_epoch_loss = epoch_loss / len_train\n",
        "            avg_eval_loss = epoch_val_loss / len_val_train\n",
        "            avg_accuracy_loss = accuracy / len_val_train\n",
        "            print('Epoch: {} avg loss = {:0.4f} eval loss = {:0.4f} ACC = {:0.4f}'.format(epoch, avg_epoch_loss, avg_eval_loss, avg_accuracy_loss))\n",
        "\n",
        "            train_loss += avg_epoch_loss\n",
        "            # torch.save(self.model.state_dict(),\n",
        "            #            os.path.join(output_folder, 'state_{}.pt'.format(epoch)))  # save the model state\n",
        "\n",
        "        avg_epoch_loss = train_loss / epochs\n",
        "        return avg_epoch_loss"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q56KhN0wjg_i"
      },
      "source": [
        "amazon_review_recurrent_classifier = AmazonReviewRecurrentClassifier(vectors_store, n_hidden=128)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6deb972fdcc1437595a0906a9910a248",
            "00fc3e5c674741ceaeb5624f41e17751",
            "664d64f7829043c2a17ecd2ca3a76eee",
            "f15df09db02d404aa0216eb1d4350b8a",
            "96fd5477430c4b6c89ec150312736b23",
            "c861684fc1cc482e9bf2adf9bd779d55",
            "5a0dc0f94d784b6f89bfe523501edb83",
            "b9e7789ccb6f404cb9fef01a77883f64"
          ]
        },
        "id": "zmaXIR2DjtCg",
        "outputId": "a16433e5-cc31-4ece-8b23-9f81af527259"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "optimizer = torch.optim.Adam(amazon_review_recurrent_classifier.parameters(), lr=0.001)\n",
        "trainer = Trainer(amazon_review_recurrent_classifier, optimizer, device)\n",
        "\n",
        "\n",
        "train_dataloader = amazon_review_rnn_dm.train_dataloader()\n",
        "avg_loss = trainer.train(train_dataloader,amazon_review_rnn_dm.val_dataloader(), epochs=150)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6deb972fdcc1437595a0906a9910a248",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=150.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 avg loss = 1.7484 eval loss = 1.7250 ACC = 0.5029\n",
            "Epoch: 1 avg loss = 1.7469 eval loss = 1.7273 ACC = 0.5029\n",
            "Epoch: 2 avg loss = 1.7458 eval loss = 1.7277 ACC = 0.5029\n",
            "Epoch: 3 avg loss = 1.7430 eval loss = 1.7296 ACC = 0.5029\n",
            "Epoch: 4 avg loss = 1.7402 eval loss = 1.7324 ACC = 0.5029\n",
            "Epoch: 5 avg loss = 1.7354 eval loss = 1.7332 ACC = 0.5029\n",
            "Epoch: 6 avg loss = 1.7288 eval loss = 1.7302 ACC = 0.5029\n",
            "Epoch: 7 avg loss = 1.7179 eval loss = 1.7395 ACC = 0.5029\n",
            "Epoch: 8 avg loss = 1.6969 eval loss = 1.7517 ACC = 0.5029\n",
            "Epoch: 9 avg loss = 1.6706 eval loss = 1.7607 ACC = 0.5029\n",
            "Epoch: 10 avg loss = 1.6439 eval loss = 1.7796 ACC = 0.5029\n",
            "Epoch: 11 avg loss = 1.6196 eval loss = 1.8555 ACC = 0.5029\n",
            "Epoch: 12 avg loss = 1.6023 eval loss = 1.8380 ACC = 0.5029\n",
            "Epoch: 13 avg loss = 1.5907 eval loss = 1.8183 ACC = 0.5029\n",
            "Epoch: 14 avg loss = 1.5718 eval loss = 2.1400 ACC = 0.5029\n",
            "Epoch: 15 avg loss = 1.5582 eval loss = 2.3061 ACC = 0.5029\n",
            "Epoch: 16 avg loss = 1.5422 eval loss = 2.4677 ACC = 0.5029\n",
            "Epoch: 17 avg loss = 1.5307 eval loss = 2.3901 ACC = 0.5029\n",
            "Epoch: 18 avg loss = 1.5206 eval loss = 2.2126 ACC = 0.5029\n",
            "Epoch: 19 avg loss = 1.5077 eval loss = 2.5712 ACC = 0.5029\n",
            "Epoch: 20 avg loss = 1.4960 eval loss = 2.8031 ACC = 0.5029\n",
            "Epoch: 21 avg loss = 1.4917 eval loss = 2.8216 ACC = 0.5029\n",
            "Epoch: 22 avg loss = 1.4908 eval loss = 3.1770 ACC = 0.5029\n",
            "Epoch: 23 avg loss = 1.4860 eval loss = 2.9176 ACC = 0.5029\n",
            "Epoch: 24 avg loss = 1.4829 eval loss = 2.7612 ACC = 0.5029\n",
            "Epoch: 25 avg loss = 1.4724 eval loss = 2.6689 ACC = 0.5029\n",
            "Epoch: 26 avg loss = 1.4650 eval loss = 2.6705 ACC = 0.5029\n",
            "Epoch: 27 avg loss = 1.4561 eval loss = 2.8180 ACC = 0.5029\n",
            "Epoch: 28 avg loss = 1.4502 eval loss = 2.9773 ACC = 0.5029\n",
            "Epoch: 29 avg loss = 1.4526 eval loss = 2.9264 ACC = 0.5029\n",
            "Epoch: 30 avg loss = 1.4497 eval loss = 3.0045 ACC = 0.5029\n",
            "Epoch: 31 avg loss = 1.4458 eval loss = 2.9727 ACC = 0.5029\n",
            "Epoch: 32 avg loss = 1.4506 eval loss = 2.8931 ACC = 0.5029\n",
            "Epoch: 33 avg loss = 1.4431 eval loss = 2.8963 ACC = 0.5029\n",
            "Epoch: 34 avg loss = 1.4428 eval loss = 3.2893 ACC = 0.5029\n",
            "Epoch: 35 avg loss = 1.4370 eval loss = 2.8983 ACC = 0.5029\n",
            "Epoch: 36 avg loss = 1.4331 eval loss = 3.0432 ACC = 0.5029\n",
            "Epoch: 37 avg loss = 1.4344 eval loss = 3.1006 ACC = 0.5029\n",
            "Epoch: 38 avg loss = 1.4258 eval loss = 2.9757 ACC = 0.5029\n",
            "Epoch: 39 avg loss = 1.4262 eval loss = 3.3123 ACC = 0.5029\n",
            "Epoch: 40 avg loss = 1.4365 eval loss = 3.5509 ACC = 0.5029\n",
            "Epoch: 41 avg loss = 1.4398 eval loss = 2.9478 ACC = 0.5029\n",
            "Epoch: 42 avg loss = 1.4261 eval loss = 3.1602 ACC = 0.5029\n",
            "Epoch: 43 avg loss = 1.4201 eval loss = 3.5085 ACC = 0.5029\n",
            "Epoch: 44 avg loss = 1.4165 eval loss = 3.3387 ACC = 0.5029\n",
            "Epoch: 45 avg loss = 1.4191 eval loss = 3.5474 ACC = 0.5029\n",
            "Epoch: 46 avg loss = 1.4258 eval loss = 3.1416 ACC = 0.5029\n",
            "Epoch: 47 avg loss = 1.4282 eval loss = 2.9761 ACC = 0.5029\n",
            "Epoch: 48 avg loss = 1.4216 eval loss = 3.1362 ACC = 0.5029\n",
            "Epoch: 49 avg loss = 1.4149 eval loss = 3.3183 ACC = 0.5029\n",
            "Epoch: 50 avg loss = 1.4135 eval loss = 3.5814 ACC = 0.5029\n",
            "Epoch: 51 avg loss = 1.4147 eval loss = 3.6072 ACC = 0.5029\n",
            "Epoch: 52 avg loss = 1.4372 eval loss = 3.4331 ACC = 0.5029\n",
            "Epoch: 53 avg loss = 1.4258 eval loss = 3.6424 ACC = 0.5029\n",
            "Epoch: 54 avg loss = 1.4199 eval loss = 3.9408 ACC = 0.5029\n",
            "Epoch: 55 avg loss = 1.4173 eval loss = 3.8344 ACC = 0.5029\n",
            "Epoch: 56 avg loss = 1.4156 eval loss = 3.7810 ACC = 0.5029\n",
            "Epoch: 57 avg loss = 1.4160 eval loss = 3.5142 ACC = 0.5029\n",
            "Epoch: 58 avg loss = 1.4160 eval loss = 3.8153 ACC = 0.5029\n",
            "Epoch: 59 avg loss = 1.4138 eval loss = 3.5348 ACC = 0.5029\n",
            "Epoch: 60 avg loss = 1.4274 eval loss = 3.3696 ACC = 0.5029\n",
            "Epoch: 61 avg loss = 1.4263 eval loss = 3.7320 ACC = 0.5029\n",
            "Epoch: 62 avg loss = 1.4232 eval loss = 3.4635 ACC = 0.5029\n",
            "Epoch: 63 avg loss = 1.4193 eval loss = 3.6599 ACC = 0.5029\n",
            "Epoch: 64 avg loss = 1.4149 eval loss = 4.1102 ACC = 0.5029\n",
            "Epoch: 65 avg loss = 1.4111 eval loss = 4.2694 ACC = 0.5029\n",
            "Epoch: 66 avg loss = 1.4105 eval loss = 4.3424 ACC = 0.5029\n",
            "Epoch: 67 avg loss = 1.4096 eval loss = 4.1797 ACC = 0.5029\n",
            "Epoch: 68 avg loss = 1.4092 eval loss = 4.2509 ACC = 0.5029\n",
            "Epoch: 69 avg loss = 1.4156 eval loss = 3.6770 ACC = 0.5029\n",
            "Epoch: 70 avg loss = 1.4424 eval loss = 2.8416 ACC = 0.5029\n",
            "Epoch: 71 avg loss = 1.4354 eval loss = 3.3558 ACC = 0.5029\n",
            "Epoch: 72 avg loss = 1.4156 eval loss = 3.7250 ACC = 0.5029\n",
            "Epoch: 73 avg loss = 1.4113 eval loss = 3.8432 ACC = 0.5029\n",
            "Epoch: 74 avg loss = 1.4097 eval loss = 3.8653 ACC = 0.5029\n",
            "Epoch: 75 avg loss = 1.4086 eval loss = 3.7416 ACC = 0.5029\n",
            "Epoch: 76 avg loss = 1.4084 eval loss = 3.9030 ACC = 0.5029\n",
            "Epoch: 77 avg loss = 1.4083 eval loss = 4.0245 ACC = 0.5029\n",
            "Epoch: 78 avg loss = 1.4082 eval loss = 4.1031 ACC = 0.5029\n",
            "Epoch: 79 avg loss = 1.4081 eval loss = 4.0498 ACC = 0.5029\n",
            "Epoch: 80 avg loss = 1.4138 eval loss = 4.2750 ACC = 0.5029\n",
            "Epoch: 81 avg loss = 1.4542 eval loss = 3.1353 ACC = 0.5029\n",
            "Epoch: 82 avg loss = 1.4311 eval loss = 2.7457 ACC = 0.5029\n",
            "Epoch: 83 avg loss = 1.4153 eval loss = 3.1988 ACC = 0.5029\n",
            "Epoch: 84 avg loss = 1.4115 eval loss = 3.7360 ACC = 0.5029\n",
            "Epoch: 85 avg loss = 1.4090 eval loss = 3.5236 ACC = 0.5029\n",
            "Epoch: 86 avg loss = 1.4080 eval loss = 3.7983 ACC = 0.5029\n",
            "Epoch: 87 avg loss = 1.4078 eval loss = 3.9212 ACC = 0.5029\n",
            "Epoch: 88 avg loss = 1.4077 eval loss = 3.9880 ACC = 0.5029\n",
            "Epoch: 89 avg loss = 1.4076 eval loss = 4.0319 ACC = 0.5029\n",
            "Epoch: 90 avg loss = 1.4075 eval loss = 4.0557 ACC = 0.5029\n",
            "Epoch: 91 avg loss = 1.4073 eval loss = 4.0418 ACC = 0.5029\n",
            "Epoch: 92 avg loss = 1.4071 eval loss = 4.1274 ACC = 0.5029\n",
            "Epoch: 93 avg loss = 1.4070 eval loss = 4.0338 ACC = 0.5029\n",
            "Epoch: 94 avg loss = 1.4069 eval loss = 4.0525 ACC = 0.5029\n",
            "Epoch: 95 avg loss = 1.4065 eval loss = 4.0774 ACC = 0.5029\n",
            "Epoch: 96 avg loss = 1.4065 eval loss = 4.1241 ACC = 0.5029\n",
            "Epoch: 97 avg loss = 1.4063 eval loss = 4.3575 ACC = 0.5029\n",
            "Epoch: 98 avg loss = 1.4664 eval loss = 2.7956 ACC = 0.5029\n",
            "Epoch: 99 avg loss = 1.4397 eval loss = 3.2228 ACC = 0.5029\n",
            "Epoch: 100 avg loss = 1.4156 eval loss = 3.6952 ACC = 0.5029\n",
            "Epoch: 101 avg loss = 1.4082 eval loss = 3.7712 ACC = 0.5029\n",
            "Epoch: 102 avg loss = 1.4071 eval loss = 3.6818 ACC = 0.5029\n",
            "Epoch: 103 avg loss = 1.4068 eval loss = 4.0740 ACC = 0.5029\n",
            "Epoch: 104 avg loss = 1.4067 eval loss = 3.9864 ACC = 0.5029\n",
            "Epoch: 105 avg loss = 1.4066 eval loss = 4.2159 ACC = 0.5029\n",
            "Epoch: 106 avg loss = 1.4065 eval loss = 4.2578 ACC = 0.5029\n",
            "Epoch: 107 avg loss = 1.4065 eval loss = 4.3144 ACC = 0.5029\n",
            "Epoch: 108 avg loss = 1.4065 eval loss = 4.3629 ACC = 0.5029\n",
            "Epoch: 109 avg loss = 1.4065 eval loss = 4.4034 ACC = 0.5029\n",
            "Epoch: 110 avg loss = 1.4065 eval loss = 4.4350 ACC = 0.5029\n",
            "Epoch: 111 avg loss = 1.4064 eval loss = 4.4713 ACC = 0.5029\n",
            "Epoch: 112 avg loss = 1.4064 eval loss = 4.4933 ACC = 0.5029\n",
            "Epoch: 113 avg loss = 1.4064 eval loss = 4.5245 ACC = 0.5029\n",
            "Epoch: 114 avg loss = 1.4064 eval loss = 4.5562 ACC = 0.5029\n",
            "Epoch: 115 avg loss = 1.4064 eval loss = 4.5708 ACC = 0.5029\n",
            "Epoch: 116 avg loss = 1.4064 eval loss = 4.5928 ACC = 0.5029\n",
            "Epoch: 117 avg loss = 1.4536 eval loss = 2.4025 ACC = 0.5029\n",
            "Epoch: 118 avg loss = 1.4601 eval loss = 2.9098 ACC = 0.5029\n",
            "Epoch: 119 avg loss = 1.4224 eval loss = 3.7805 ACC = 0.5029\n",
            "Epoch: 120 avg loss = 1.4133 eval loss = 4.0034 ACC = 0.5029\n",
            "Epoch: 121 avg loss = 1.4115 eval loss = 3.8039 ACC = 0.5029\n",
            "Epoch: 122 avg loss = 1.4113 eval loss = 4.0660 ACC = 0.5029\n",
            "Epoch: 123 avg loss = 1.4111 eval loss = 4.2888 ACC = 0.5029\n",
            "Epoch: 124 avg loss = 1.4112 eval loss = 4.3753 ACC = 0.5029\n",
            "Epoch: 125 avg loss = 1.4113 eval loss = 4.4396 ACC = 0.5029\n",
            "Epoch: 126 avg loss = 1.4113 eval loss = 4.4566 ACC = 0.5029\n",
            "Epoch: 127 avg loss = 1.4112 eval loss = 4.4940 ACC = 0.5029\n",
            "Epoch: 128 avg loss = 1.4114 eval loss = 4.5746 ACC = 0.5029\n",
            "Epoch: 129 avg loss = 1.4113 eval loss = 4.6127 ACC = 0.5029\n",
            "Epoch: 130 avg loss = 1.4113 eval loss = 4.6488 ACC = 0.5029\n",
            "Epoch: 131 avg loss = 1.4113 eval loss = 4.6835 ACC = 0.5029\n",
            "Epoch: 132 avg loss = 1.4114 eval loss = 4.7133 ACC = 0.5029\n",
            "Epoch: 133 avg loss = 1.4114 eval loss = 4.7422 ACC = 0.5029\n",
            "Epoch: 134 avg loss = 1.4114 eval loss = 4.7679 ACC = 0.5029\n",
            "Epoch: 135 avg loss = 1.4114 eval loss = 4.8016 ACC = 0.5029\n",
            "Epoch: 136 avg loss = 1.4115 eval loss = 4.8302 ACC = 0.5029\n",
            "Epoch: 137 avg loss = 1.4115 eval loss = 4.8522 ACC = 0.5029\n",
            "Epoch: 138 avg loss = 1.4115 eval loss = 4.8802 ACC = 0.5029\n",
            "Epoch: 139 avg loss = 1.4115 eval loss = 4.8914 ACC = 0.5029\n",
            "Epoch: 140 avg loss = 1.4115 eval loss = 4.8761 ACC = 0.5029\n",
            "Epoch: 141 avg loss = 1.4115 eval loss = 4.9124 ACC = 0.5029\n",
            "Epoch: 142 avg loss = 1.4116 eval loss = 4.9281 ACC = 0.5029\n",
            "Epoch: 143 avg loss = 1.4116 eval loss = 4.9716 ACC = 0.5029\n",
            "Epoch: 144 avg loss = 1.4116 eval loss = 4.9517 ACC = 0.5029\n",
            "Epoch: 145 avg loss = 1.4116 eval loss = 4.9173 ACC = 0.5029\n",
            "Epoch: 146 avg loss = 1.4116 eval loss = 5.0312 ACC = 0.5029\n",
            "Epoch: 147 avg loss = 1.4117 eval loss = 4.8743 ACC = 0.5029\n",
            "Epoch: 148 avg loss = 1.4117 eval loss = 5.0214 ACC = 0.5029\n",
            "Epoch: 149 avg loss = 1.4118 eval loss = 5.1035 ACC = 0.5029\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}